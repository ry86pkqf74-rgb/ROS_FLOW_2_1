name: Agent Evaluation Harness

on:
  pull_request:
    paths:
      - 'agents/**'
      - 'services/worker/src/agents/**'
      - 'packages/ai-agents/src/**'
      - 'tests/eval/**'
      - '.github/workflows/eval-harness.yml'
  push:
    branches:
      - main
      - develop
  schedule:
    # Nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Evaluation mode'
        required: true
        default: 'dry-run'
        type: choice
        options:
          - dry-run
          - full

jobs:
  eval-schema-only:
    name: Schema Validation (P0)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r researchflow-production-main/tests/eval/requirements.txt

      - name: Run schema-only validation
        run: |
          cd researchflow-production-main
          python tests/eval/run_eval.py --all --dry-run --output tests/eval/results/
        env:
          EVAL_MODE: ci

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-schema-${{ github.sha }}
          path: researchflow-production-main/tests/eval/results/
          retention-days: 30

      - name: Check thresholds
        run: |
          cd researchflow-production-main
          python tests/eval/run_eval.py --check-results tests/eval/results/

  eval-extractions:
    name: Extraction Agent Evaluation (P1)
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r researchflow-production-main/tests/eval/requirements.txt

      - name: Run extraction evaluation (dry-run in CI)
        run: |
          cd researchflow-production-main
          python tests/eval/run_eval.py \
            --agent stage2-extract \
            --dataset tests/eval/datasets/golden-extractions.jsonl \
            --output tests/eval/results/ \
            --dry-run
        env:
          EVAL_MODE: ci
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-extractions-${{ github.sha }}
          path: researchflow-production-main/tests/eval/results/
          retention-days: 30

      - name: Check thresholds
        run: |
          cd researchflow-production-main
          python tests/eval/run_eval.py --check-results tests/eval/results/

  eval-synthesis:
    name: Synthesis Agent Evaluation (P2)
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && github.event.inputs.mode == 'full')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r researchflow-production-main/tests/eval/requirements.txt

      - name: Run synthesis evaluation (dry-run in CI)
        run: |
          cd researchflow-production-main
          python tests/eval/run_eval.py \
            --agent stage2-synthesize \
            --dataset tests/eval/datasets/golden-synthesis.jsonl \
            --output tests/eval/results/ \
            --dry-run
        env:
          EVAL_MODE: ci
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-synthesis-${{ github.sha }}
          path: researchflow-production-main/tests/eval/results/
          retention-days: 30

      - name: Check thresholds
        run: |
          cd researchflow-production-main
          python tests/eval/run_eval.py --check-results tests/eval/results/

  eval-summary:
    name: Evaluation Summary
    runs-on: ubuntu-latest
    needs: [eval-schema-only, eval-extractions, eval-synthesis]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: eval-results-*
          path: all-results/

      - name: Generate summary
        run: |
          echo "# Evaluation Harness Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Schema Validation | ${{ needs.eval-schema-only.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Extraction Agents | ${{ needs.eval-extractions.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Synthesis Agents | ${{ needs.eval-synthesis.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Results artifacts retained for 30 days." >> $GITHUB_STEP_SUMMARY

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Evaluation Harness Results')
            );
            
            const body = `## ðŸ§ª Evaluation Harness Results
            
            | Job | Status |
            |-----|--------|
            | Schema Validation | ${{ needs.eval-schema-only.result }} |
            | Extraction Agents | ${{ needs.eval-extractions.result }} |
            | Synthesis Agents | ${{ needs.eval-synthesis.result }} |
            
            See [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) for details.
            `;
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
