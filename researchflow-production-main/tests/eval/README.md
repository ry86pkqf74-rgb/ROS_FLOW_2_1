# Evaluation Harness

Automated evaluation for ResearchFlow agents. See [EVAL_HARNESS_ROADMAP.md](../../EVAL_HARNESS_ROADMAP.md) for full design.

## Quick Start

```bash
# Install deps
pip install -r requirements.txt

# P0: Schema-only validation (no agents needed, no LLM calls)
python run_eval.py --all --dry-run

# Single agent with dataset
python run_eval.py --agent stage2-extract --dataset datasets/golden-extractions.jsonl

# Check saved results
python run_eval.py --check-results results/
```

## Structure

```
tests/eval/
â”œâ”€â”€ run_eval.py          # CLI entry point
â”œâ”€â”€ harness.py           # Core evaluation loop
â”œâ”€â”€ conftest.py          # Pytest fixtures
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ schema_validity.py   # JSON Schema validation (P0)
â”‚   â”œâ”€â”€ latency.py           # Wall-clock timing
â”‚   â”œâ”€â”€ groundedness.py      # Rouge-L F1 scoring
â”‚   â””â”€â”€ cost.py              # Token cost estimation
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ schemas/             # One .json per agent
â””â”€â”€ results/                 # gitignored; CI uploads as artifact
```

## Phases

| Phase | What | Status |
|-------|------|--------|
| P0 â€” Schema only | Validate response shapes, no LLM | âœ… This PR |
| P1 â€” Extractions | Golden dataset + groundedness | ðŸ”œ Next |
| P2 â€” Synthesis   | Cover synthesize + verify | Planned |
| P3 â€” Manuscripts | FAVES + Rouge-L for writers | Planned |
| P4 â€” Cost        | Baseline + regression alerts | Planned |
