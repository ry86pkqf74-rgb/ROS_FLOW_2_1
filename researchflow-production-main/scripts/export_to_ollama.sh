#!/bin/bash
# =============================================================================
# Export Fine-Tuned Model to Ollama
# =============================================================================
# Converts LoRA fine-tuned models to GGUF format and imports into Ollama.
#
# Usage:
#   ./scripts/export_to_ollama.sh models/research-refiner/final [model-name]
#
# Linear Issue: ROS-82
# =============================================================================

set -e

MODEL_PATH="${1:-models/research-refiner/final}"
MODEL_NAME="${2:-research-refiner}"
QUANTIZATION="${3:-q4_k_m}"

echo "=== ResearchFlow Model Export ==="
echo "Model Path: $MODEL_PATH"
echo "Model Name: $MODEL_NAME"
echo "Quantization: $QUANTIZATION"

# Check prerequisites
if [ ! -d "$MODEL_PATH" ]; then
    echo "Error: Model path does not exist: $MODEL_PATH"
    exit 1
fi

if ! command -v ollama &> /dev/null; then
    echo "Error: Ollama not installed. Install from: https://ollama.ai"
    exit 1
fi

# Create output directory
OUTPUT_DIR="${MODEL_PATH}/gguf"
mkdir -p "$OUTPUT_DIR"

echo ""
echo "Step 1: Converting to GGUF format..."

# Use llama.cpp convert script (assumes it's installed)
python -m llama_cpp.convert \
    --input "$MODEL_PATH" \
    --output "$OUTPUT_DIR/${MODEL_NAME}-f16.gguf" \
    --outtype f16 2>/dev/null || \
    echo "Note: Using alternative conversion method"

# Alternative: use transformers directly if llama_cpp not available
if [ ! -f "$OUTPUT_DIR/${MODEL_NAME}-f16.gguf" ]; then
    echo "Using HuggingFace to GGUF conversion..."
    pip install -q gguf
    python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained('$MODEL_PATH', torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained('$MODEL_PATH')
model.save_pretrained('$OUTPUT_DIR/hf_export', safe_serialization=True)
print('Model exported for manual GGUF conversion')
"
fi

echo ""
echo "Step 2: Creating Modelfile..."

cat > "$OUTPUT_DIR/Modelfile" << 'EOF'
# ResearchFlow Fine-Tuned Model
# Generated by export_to_ollama.sh

FROM ./MODEL_NAME-q4_k_m.gguf

# System prompt for research tasks
SYSTEM """You are a specialized research assistant fine-tuned for clinical research workflows. You excel at:
- Manuscript refinement and scientific writing
- Statistical analysis interpretation
- Data quality assessment
- IRB compliance review

Always provide accurate, well-referenced responses following scientific standards."""

# Parameters optimized for research tasks
PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_predict 4096
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|im_end|>"

# Template for Llama-style models
TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
EOF

# Replace placeholder with actual model name
sed -i "s/MODEL_NAME/${MODEL_NAME}/g" "$OUTPUT_DIR/Modelfile"

echo ""
echo "Step 3: Importing to Ollama..."

cd "$OUTPUT_DIR"
ollama create "$MODEL_NAME" -f Modelfile

echo ""
echo "=== Export Complete ==="
echo "Model available as: ollama run $MODEL_NAME"
echo ""
echo "Test with:"
echo "  ollama run $MODEL_NAME 'Refine this methods section: We used t-test.'"
