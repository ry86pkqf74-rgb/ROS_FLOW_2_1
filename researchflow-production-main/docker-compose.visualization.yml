# =============================================================================
# Docker Compose - Visualization System Optimized Configuration
# =============================================================================
# This extends the base docker-compose.yml with visualization-specific optimizations

version: '3.8'

services:
  # =============================================================================
  # Orchestrator Service - Enhanced for Visualization
  # =============================================================================
  orchestrator:
    environment:
      # Visualization Configuration
      - VIZ_MAX_DATA_POINTS=50000
      - VIZ_MAX_CONCURRENT=10
      - VIZ_TIMEOUT_MS=60000
      - VIZ_DEFAULT_DPI=300
      - VIZ_ENABLE_CACHE=true
      - VIZ_CACHE_EXPIRY=24
      - VIZ_RATE_LIMIT_PER_MIN=30
      - VIZ_OPTIMIZE_IMAGES=true
      - VIZ_ENABLE_METRICS=true
      - VIZ_PHI_SCANNING=true
      
      # Worker connection
      - WORKER_URL=http://worker:8000
      - REDIS_URL=redis://redis:6379
      
      # Performance tuning
      - NODE_OPTIONS=--max-old-space-size=2048
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/visualization/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
      
    depends_on:
      redis:
        condition: service_healthy
      worker:
        condition: service_healthy

  # =============================================================================
  # Worker Service - Optimized for Chart Generation
  # =============================================================================  
  worker:
    environment:
      # Python optimization for visualization
      - PYTHONUNBUFFERED=1
      - MALLOC_TRIM_THRESHOLD=100000
      
      # Matplotlib configuration
      - MPLBACKEND=Agg
      - MPLCONFIGDIR=/tmp/matplotlib
      
      # Memory optimization
      - PYTHONHASHSEED=0
      
    volumes:
      # Mount for temporary chart files
      - worker_charts:/app/charts
      
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/api/visualization/health').raise_for_status()"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
      
    deploy:
      resources:
        limits:
          cpus: '2.0'      # 2 CPU cores for chart generation
          memory: 4G        # 4GB RAM for large datasets
        reservations:
          cpus: '1.0'
          memory: 2G
          
    restart: unless-stopped

  # =============================================================================
  # Redis - Optimized for Visualization Caching
  # =============================================================================
  redis:
    command: >
      redis-server
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
      --appendonly yes
      --tcp-keepalive 60
      --timeout 300
      
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
      
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M

  # =============================================================================
  # PostgreSQL - Optimized for Figure Storage
  # =============================================================================
  postgres:
    environment:
      # Optimization for binary data storage
      - POSTGRES_SHARED_PRELOAD_LIBRARIES=pg_stat_statements
      
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=8
      -c max_parallel_workers_per_gather=4
      -c max_parallel_workers=8
      -c max_parallel_maintenance_workers=4
      
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-researchflow}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # =============================================================================
  # Monitoring Services (Optional)
  # =============================================================================
  
  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: researchflow_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    profiles:
      - monitoring
    restart: unless-stopped
    
  # Grafana for visualization monitoring dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: researchflow_grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    profiles:
      - monitoring
    restart: unless-stopped
    depends_on:
      - prometheus

# =============================================================================
# Volumes
# =============================================================================
volumes:
  worker_charts:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  default:
    name: researchflow_visualization
    driver: bridge

# =============================================================================
# Usage Instructions
# =============================================================================
#
# 1. Basic visualization deployment:
#    docker compose -f docker-compose.yml -f docker-compose.visualization.yml up -d
#
# 2. With monitoring:
#    docker compose -f docker-compose.yml -f docker-compose.visualization.yml --profile monitoring up -d
#
# 3. Production deployment:
#    docker compose -f docker-compose.yml -f docker-compose.visualization.yml -f docker-compose.prod.yml up -d
#
# 4. Check health:
#    curl http://localhost:3001/api/visualization/health
#
# 5. View metrics:
#    curl http://localhost:3001/api/visualization/metrics
#
# 6. Access monitoring:
#    Grafana: http://localhost:3000 (admin/admin123)
#    Prometheus: http://localhost:9090
#
# =============================================================================
# Resource Requirements
# =============================================================================
#
# Minimum:
#   - CPU: 4 cores
#   - RAM: 8GB 
#   - Disk: 20GB
#
# Recommended:
#   - CPU: 8 cores
#   - RAM: 16GB
#   - Disk: 100GB SSD
#
# High-Traffic:
#   - CPU: 16 cores
#   - RAM: 32GB
#   - Disk: 500GB NVMe
#
# =============================================================================