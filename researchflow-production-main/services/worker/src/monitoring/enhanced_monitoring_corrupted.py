"""
Enhanced System Monitoring and Performance Optimization

Advanced monitoring system that extends the performance dashboard with:
- Real-time metrics collection
- Predictive performance analytics
- Automated optimization recommendations
- Resource usage optimization
- Memory leak detection
- Database query optimization
- Cache performance tuning

Author: Performance Optimization Team
"""

import logging
import asyncio
import time
import psutil
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from collections import deque, defaultdict
import numpy as np
import json
from contextlib import contextmanager
import sqlite3
from pathlib import Path

# Import base monitoring
from .performance_dashboard import PerformanceMonitor, PerformanceMetric, SystemHealth

logger = logging.getLogger(__name__)

@dataclass
class OptimizationRecommendation:
    """Performance optimization recommendation."""
    
    recommendation_id: str
    category: str  # "memory", "cpu", "database", "cache", "network"
    priority: str  # "low", "medium", "high", "critical"
    
    title: str
    description: str
    impact_estimate: str  # "5-10% improvement", "50MB memory saved"
    implementation_effort: str  # "low", "medium", "high"
    
    # Technical details
    current_metric: float
    target_metric: float
    threshold_breached: bool
    
    # Actions
    suggested_actions: List[str] = field(default_factory=list)
    configuration_changes: Dict[str, Any] = field(default_factory=dict)
    
    # Tracking
    created_at: datetime = field(default_factory=datetime.now)
    applied_at: Optional[datetime] = None
    verified_at: Optional[datetime] = None

@dataclass
class ResourceUsage:
    """Detailed resource usage metrics."""
    
    timestamp: datetime
    
    # CPU metrics
    cpu_percent: float
    cpu_cores: int
    load_average: List[float]
    
    # Memory metrics
    memory_total: int
    memory_used: int
    memory_percent: float
    memory_available: int
    
    # Disk metrics
    disk_total: int
    disk_used: int
    disk_percent: float
    disk_io_read: int
    disk_io_write: int
    
    # Network metrics
    network_bytes_sent: int
    network_bytes_recv: int
    network_packets_sent: int
    network_packets_recv: int
    
    # Process-specific metrics
    process_memory: int
    process_cpu: float
    open_files: int
    threads: int

class EnhancedMonitor:
    """
    Enhanced monitoring system with optimization intelligence.
    
    Extends base performance monitoring with:
    - Predictive analytics
    - Automated optimization
    - Resource trend analysis
    - Performance bottleneck detection
    """
    
    def __init__(self, 
                 base_monitor: Optional[PerformanceMonitor] = None,
                 optimization_enabled: bool = True):
        \"\"\"Initialize enhanced monitoring system.\"\"\"\n        self.base_monitor = base_monitor or PerformanceMonitor()\n        self.optimization_enabled = optimization_enabled\n        \n        # Enhanced storage\n        self.resource_metrics: deque = deque(maxlen=10080)  # 1 week at 1-minute intervals\n        self.optimization_history: List[OptimizationRecommendation] = []\n        \n        # Analysis state\n        self.last_optimization_check = datetime.now()\n        self.trend_analysis_cache: Dict[str, Any] = {}\n        \n        # Performance baselines\n        self.performance_baselines = {\n            \"response_time_p95\": 2.0,  # 2 seconds\n            \"memory_usage_threshold\": 0.80,  # 80%\n            \"cpu_usage_threshold\": 0.75,  # 75%\n            \"error_rate_threshold\": 0.01,  # 1%\n            \"disk_usage_threshold\": 0.85,  # 85%\n        }\n        \n        # Optimization rules\n        self.optimization_rules = self._setup_optimization_rules()\n        \n        # Database for persistent storage\n        self.db_path = Path(\"./monitoring_enhanced.db\")\n        self._setup_database()\n        \n        # Background monitoring\n        self.enhanced_monitoring_active = False\n        self.enhanced_monitoring_thread = None\n        \n        logger.info(\"Enhanced Monitor initialized\")\n    \n    def _setup_optimization_rules(self) -> List[Dict[str, Any]]:\n        \"\"\"Setup automated optimization rules.\"\"\"\n        return [\n            {\n                \"id\": \"high_memory_usage\",\n                \"condition\": lambda metrics: metrics[-1].memory_percent > 80,\n                \"recommendation\": {\n                    \"category\": \"memory\",\n                    \"priority\": \"high\",\n                    \"title\": \"High Memory Usage Detected\",\n                    \"description\": \"System memory usage exceeds 80% threshold\",\n                    \"suggested_actions\": [\n                        \"Enable memory caching optimization\",\n                        \"Increase garbage collection frequency\",\n                        \"Review memory-intensive operations\",\n                        \"Consider horizontal scaling\"\n                    ]\n                }\n            },\n            {\n                \"id\": \"slow_response_times\",\n                \"condition\": lambda metrics: self._get_avg_response_time(300) > 3.0,  # 5 min average > 3s\n                \"recommendation\": {\n                    \"category\": \"performance\",\n                    \"priority\": \"medium\",\n                    \"title\": \"Slow Response Times Detected\",\n                    \"description\": \"Average response time exceeds 3 seconds\",\n                    \"suggested_actions\": [\n                        \"Enable request caching\",\n                        \"Optimize database queries\",\n                        \"Review API endpoint efficiency\",\n                        \"Consider CDN for static content\"\n                    ]\n                }\n            },\n            {\n                \"id\": \"high_cpu_usage\",\n                \"condition\": lambda metrics: metrics[-1].cpu_percent > 75,\n                \"recommendation\": {\n                    \"category\": \"cpu\",\n                    \"priority\": \"medium\",\n                    \"title\": \"High CPU Usage Detected\",\n                    \"description\": \"CPU usage exceeds 75% threshold\",\n                    \"suggested_actions\": [\n                        \"Optimize CPU-intensive algorithms\",\n                        \"Enable async processing for heavy tasks\",\n                        \"Review background task scheduling\",\n                        \"Consider process optimization\"\n                    ]\n                }\n            },\n            {\n                \"id\": \"database_performance\",\n                \"condition\": lambda metrics: self._get_avg_db_query_time() > 1.0,\n                \"recommendation\": {\n                    \"category\": \"database\",\n                    \"priority\": \"high\",\n                    \"title\": \"Slow Database Queries\",\n                    \"description\": \"Average database query time exceeds 1 second\",\n                    \"suggested_actions\": [\n                        \"Add database indexes\",\n                        \"Optimize query patterns\",\n                        \"Enable connection pooling\",\n                        \"Review database schema design\"\n                    ]\n                }\n            }\n        ]\n    \n    def _setup_database(self) -> None:\n        \"\"\"Setup SQLite database for persistent monitoring data.\"\"\"\n        try:\n            conn = sqlite3.connect(str(self.db_path))\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS resource_metrics (\n                    timestamp TEXT,\n                    cpu_percent REAL,\n                    memory_percent REAL,\n                    disk_percent REAL,\n                    network_bytes_sent INTEGER,\n                    network_bytes_recv INTEGER\n                )\n            \"\"\")\n            \n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS optimization_recommendations (\n                    id TEXT PRIMARY KEY,\n                    category TEXT,\n                    priority TEXT,\n                    title TEXT,\n                    description TEXT,\n                    current_metric REAL,\n                    target_metric REAL,\n                    created_at TEXT,\n                    applied_at TEXT,\n                    verified_at TEXT\n                )\n            \"\"\")\n            \n            conn.commit()\n            conn.close()\n            \n        except Exception as e:\n            logger.error(f\"Error setting up monitoring database: {e}\")\n    \n    def start_enhanced_monitoring(self) -> None:\n        \"\"\"Start enhanced monitoring with optimization analysis.\"\"\"\n        if not self.enhanced_monitoring_active:\n            self.enhanced_monitoring_active = True\n            self.enhanced_monitoring_thread = threading.Thread(\n                target=self._enhanced_monitoring_loop, \n                daemon=True\n            )\n            self.enhanced_monitoring_thread.start()\n            \n            # Also start base monitoring\n            self.base_monitor.start_monitoring()\n            \n            logger.info(\"Enhanced monitoring started\")\n    \n    def stop_enhanced_monitoring(self) -> None:\n        \"\"\"Stop enhanced monitoring.\"\"\"\n        self.enhanced_monitoring_active = False\n        if self.enhanced_monitoring_thread:\n            self.enhanced_monitoring_thread.join()\n        \n        self.base_monitor.stop_monitoring()\n        logger.info(\"Enhanced monitoring stopped\")\n    \n    def _enhanced_monitoring_loop(self) -> None:\n        \"\"\"Enhanced monitoring loop with optimization analysis.\"\"\"\n        while self.enhanced_monitoring_active:\n            try:\n                # Collect detailed resource metrics\n                resource_usage = self._collect_resource_metrics()\n                self.resource_metrics.append(resource_usage)\n                \n                # Store in database for persistence\n                self._store_resource_metrics(resource_usage)\n                \n                # Check for optimization opportunities\n                if self.optimization_enabled:\n                    await asyncio.run(self._check_optimization_opportunities())\n                \n                # Trend analysis (every 10 minutes)\n                if (datetime.now() - self.last_optimization_check).total_seconds() > 600:\n                    await asyncio.run(self._perform_trend_analysis())\n                    self.last_optimization_check = datetime.now()\n                \n            except Exception as e:\n                logger.error(f\"Error in enhanced monitoring loop: {e}\")\n            \n            # Sleep for 30 seconds (more frequent than base monitor)\n            time.sleep(30)\n    \n    def _collect_resource_metrics(self) -> ResourceUsage:\n        \"\"\"Collect detailed resource usage metrics.\"\"\"\n        try:\n            # CPU metrics\n            cpu_percent = psutil.cpu_percent(interval=1)\n            cpu_count = psutil.cpu_count()\n            \n            # Load average (Unix systems)\n            try:\n                load_avg = psutil.getloadavg()\n            except:\n                load_avg = [0, 0, 0]\n            \n            # Memory metrics\n            memory = psutil.virtual_memory()\n            \n            # Disk metrics\n            disk = psutil.disk_usage('/')\n            disk_io = psutil.disk_io_counters()\n            \n            # Network metrics\n            network = psutil.net_io_counters()\n            \n            # Current process metrics\n            process = psutil.Process()\n            process_info = process.memory_info()\n            \n            return ResourceUsage(\n                timestamp=datetime.now(),\n                cpu_percent=cpu_percent,\n                cpu_cores=cpu_count,\n                load_average=list(load_avg),\n                memory_total=memory.total,\n                memory_used=memory.used,\n                memory_percent=memory.percent,\n                memory_available=memory.available,\n                disk_total=disk.total,\n                disk_used=disk.used,\n                disk_percent=disk.percent,\n                disk_io_read=disk_io.read_bytes if disk_io else 0,\n                disk_io_write=disk_io.write_bytes if disk_io else 0,\n                network_bytes_sent=network.bytes_sent if network else 0,\n                network_bytes_recv=network.bytes_recv if network else 0,\n                network_packets_sent=network.packets_sent if network else 0,\n                network_packets_recv=network.packets_recv if network else 0,\n                process_memory=process_info.rss,\n                process_cpu=process.cpu_percent(),\n                open_files=process.num_fds() if hasattr(process, 'num_fds') else 0,\n                threads=process.num_threads()\n            )\n        \n        except Exception as e:\n            logger.error(f\"Error collecting resource metrics: {e}\")\n            return ResourceUsage(\n                timestamp=datetime.now(),\n                cpu_percent=0, cpu_cores=1, load_average=[0, 0, 0],\n                memory_total=0, memory_used=0, memory_percent=0, memory_available=0,\n                disk_total=0, disk_used=0, disk_percent=0, disk_io_read=0, disk_io_write=0,\n                network_bytes_sent=0, network_bytes_recv=0, network_packets_sent=0, network_packets_recv=0,\n                process_memory=0, process_cpu=0, open_files=0, threads=0\n            )\n    \n    def _store_resource_metrics(self, usage: ResourceUsage) -> None:\n        \"\"\"Store resource metrics in database.\"\"\"\n        try:\n            conn = sqlite3.connect(str(self.db_path))\n            conn.execute(\"\"\"\n                INSERT INTO resource_metrics \n                (timestamp, cpu_percent, memory_percent, disk_percent, network_bytes_sent, network_bytes_recv)\n                VALUES (?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                usage.timestamp.isoformat(),\n                usage.cpu_percent,\n                usage.memory_percent,\n                usage.disk_percent,\n                usage.network_bytes_sent,\n                usage.network_bytes_recv\n            ))\n            conn.commit()\n            conn.close()\n        \n        except Exception as e:\n            logger.error(f\"Error storing resource metrics: {e}\")\n    \n    async def _check_optimization_opportunities(self) -> None:\n        \"\"\"Check for optimization opportunities using rules.\"\"\"\n        try:\n            if not self.resource_metrics:\n                return\n            \n            current_metrics = list(self.resource_metrics)\n            \n            for rule in self.optimization_rules:\n                try:\n                    # Check rule condition\n                    if rule[\"condition\"](current_metrics):\n                        # Generate recommendation\n                        recommendation = self._create_recommendation_from_rule(rule)\n                        \n                        # Check if similar recommendation exists recently\n                        if not self._has_recent_recommendation(recommendation.category):\n                            self.optimization_history.append(recommendation)\n                            await self._notify_optimization_opportunity(recommendation)\n                            \n                            logger.info(f\"Optimization opportunity detected: {recommendation.title}\")\n                \n                except Exception as rule_error:\n                    logger.error(f\"Error checking optimization rule {rule['id']}: {rule_error}\")\n        \n        except Exception as e:\n            logger.error(f\"Error checking optimization opportunities: {e}\")\n    \n    def _create_recommendation_from_rule(self, rule: Dict[str, Any]) -> OptimizationRecommendation:\n        \"\"\"Create optimization recommendation from rule.\"\"\"\n        rec_data = rule[\"recommendation\"]\n        \n        # Get current metric value\n        current_metric = 0.0\n        if rec_data[\"category\"] == \"memory\":\n            current_metric = self.resource_metrics[-1].memory_percent\n        elif rec_data[\"category\"] == \"cpu\":\n            current_metric = self.resource_metrics[-1].cpu_percent\n        \n        return OptimizationRecommendation(\n            recommendation_id=f\"{rule['id']}_{int(time.time())}\",\n            category=rec_data[\"category\"],\n            priority=rec_data[\"priority\"],\n            title=rec_data[\"title\"],\n            description=rec_data[\"description\"],\n            impact_estimate=\"Estimated improvement available\",\n            implementation_effort=\"low\",\n            current_metric=current_metric,\n            target_metric=current_metric * 0.8,  # 20% improvement target\n            threshold_breached=True,\n            suggested_actions=rec_data[\"suggested_actions\"]\n        )\n    \n    def _has_recent_recommendation(self, category: str, hours: int = 1) -> bool:\n        \"\"\"Check if similar recommendation was made recently.\"\"\"\n        cutoff = datetime.now() - timedelta(hours=hours)\n        \n        for rec in self.optimization_history:\n            if rec.category == category and rec.created_at >= cutoff:\n                return True\n        \n        return False\n    \n    async def _notify_optimization_opportunity(self, recommendation: OptimizationRecommendation) -> None:\n        \"\"\"Notify about optimization opportunity.\"\"\"\n        try:\n            # Store in database\n            conn = sqlite3.connect(str(self.db_path))\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO optimization_recommendations \n                (id, category, priority, title, description, current_metric, target_metric, created_at)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", (\n                recommendation.recommendation_id,\n                recommendation.category,\n                recommendation.priority,\n                recommendation.title,\n                recommendation.description,\n                recommendation.current_metric,\n                recommendation.target_metric,\n                recommendation.created_at.isoformat()\n            ))\n            conn.commit()\n            conn.close()\n            \n            # Log recommendation\n            logger.warning(\n                f\"OPTIMIZATION OPPORTUNITY [{recommendation.priority.upper()}]: \"\n                f\"{recommendation.title} - {recommendation.description}\"\n            )\n        \n        except Exception as e:\n            logger.error(f\"Error notifying optimization opportunity: {e}\")\n    \n    async def _perform_trend_analysis(self) -> None:\n        \"\"\"Perform trend analysis and predictive analytics.\"\"\"\n        try:\n            if len(self.resource_metrics) < 10:\n                return  # Need more data points\n            \n            # Get recent metrics (last hour)\n            recent_metrics = [\n                m for m in self.resource_metrics \n                if (datetime.now() - m.timestamp).total_seconds() <= 3600\n            ]\n            \n            if not recent_metrics:\n                return\n            \n            # Analyze trends\n            trends = {\n                \"memory_trend\": self._calculate_trend([m.memory_percent for m in recent_metrics]),\n                \"cpu_trend\": self._calculate_trend([m.cpu_percent for m in recent_metrics]),\n                \"disk_trend\": self._calculate_trend([m.disk_percent for m in recent_metrics])\n            }\n            \n            # Detect concerning trends\n            for metric, trend in trends.items():\n                if trend > 0.5:  # Positive trend indicates increasing usage\n                    await self._generate_trend_alert(metric, trend)\n            \n            # Update cache\n            self.trend_analysis_cache = {\n                \"last_analysis\": datetime.now().isoformat(),\n                \"trends\": trends,\n                \"data_points\": len(recent_metrics)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error in trend analysis: {e}\")\n    \n    def _calculate_trend(self, values: List[float]) -> float:\n        \"\"\"Calculate trend slope for values.\"\"\"\n        if len(values) < 2:\n            return 0.0\n        \n        x = np.arange(len(values))\n        y = np.array(values)\n        \n        # Linear regression slope\n        slope = np.polyfit(x, y, 1)[0]\n        return slope\n    \n    async def _generate_trend_alert(self, metric: str, trend: float) -> None:\n        \"\"\"Generate alert for concerning trends.\"\"\"\n        try:\n            alert_message = f\"Concerning trend detected for {metric}: slope={trend:.3f}\"\n            logger.warning(alert_message)\n            \n            # Could integrate with alerting system here\n            \n        except Exception as e:\n            logger.error(f\"Error generating trend alert: {e}\")\n    \n    def _get_avg_response_time(self, seconds: int = 300) -> float:\n        \"\"\"Get average response time from base monitor.\"\"\"\n        try:\n            cutoff = datetime.now() - timedelta(seconds=seconds)\n            recent_metrics = [\n                m for m in self.base_monitor.metrics \n                if m.timestamp >= cutoff and m.metric_name == \"response_time\"\n            ]\n            \n            if not recent_metrics:\n                return 0.0\n            \n            return np.mean([m.value for m in recent_metrics])\n        \n        except Exception as e:\n            logger.error(f\"Error calculating average response time: {e}\")\n            return 0.0\n    \n    def _get_avg_db_query_time(self) -> float:\n        \"\"\"Get average database query time from base monitor.\"\"\"\n        try:\n            # Get recent database-related metrics\n            cutoff = datetime.now() - timedelta(minutes=5)\n            db_metrics = [\n                m for m in self.base_monitor.metrics \n                if m.timestamp >= cutoff and \"database\" in (m.operation_type or \"\")\n            ]\n            \n            if not db_metrics:\n                return 0.0\n            \n            return np.mean([m.value for m in db_metrics])\n        \n        except Exception as e:\n            logger.error(f\"Error calculating average database query time: {e}\")\n            return 0.0\n    \n    def get_optimization_recommendations(self) -> List[Dict[str, Any]]:\n        \"\"\"Get current optimization recommendations.\"\"\"\n        try:\n            # Get recent recommendations (last 24 hours)\n            cutoff = datetime.now() - timedelta(hours=24)\n            recent_recs = [\n                rec for rec in self.optimization_history \n                if rec.created_at >= cutoff and not rec.applied_at\n            ]\n            \n            # Sort by priority\n            priority_order = {\"critical\": 0, \"high\": 1, \"medium\": 2, \"low\": 3}\n            recent_recs.sort(key=lambda r: priority_order.get(r.priority, 3))\n            \n            return [{\n                \"id\": rec.recommendation_id,\n                \"category\": rec.category,\n                \"priority\": rec.priority,\n                \"title\": rec.title,\n                \"description\": rec.description,\n                \"impact_estimate\": rec.impact_estimate,\n                \"suggested_actions\": rec.suggested_actions,\n                \"created_at\": rec.created_at.isoformat()\n            } for rec in recent_recs]\n        \n        except Exception as e:\n            logger.error(f\"Error getting optimization recommendations: {e}\")\n            return []\n    \n    def get_performance_insights(self) -> Dict[str, Any]:\n        \"\"\"Get advanced performance insights.\"\"\"\n        try:\n            if not self.resource_metrics:\n                return {\"status\": \"insufficient_data\"}\n            \n            # Calculate performance metrics over different time windows\n            now = datetime.now()\n            windows = {\n                \"5min\": 300,\n                \"1hour\": 3600,\n                \"24hours\": 86400\n            }\n            \n            insights = {}\n            \n            for window_name, seconds in windows.items():\n                cutoff = now - timedelta(seconds=seconds)\n                window_metrics = [\n                    m for m in self.resource_metrics \n                    if m.timestamp >= cutoff\n                ]\n                \n                if window_metrics:\n                    insights[window_name] = {\n                        \"avg_cpu\": np.mean([m.cpu_percent for m in window_metrics]),\n                        \"max_cpu\": np.max([m.cpu_percent for m in window_metrics]),\n                        \"avg_memory\": np.mean([m.memory_percent for m in window_metrics]),\n                        \"max_memory\": np.max([m.memory_percent for m in window_metrics]),\n                        \"data_points\": len(window_metrics)\n                    }\n            \n            # Add trend information\n            insights[\"trends\"] = self.trend_analysis_cache\n            \n            # Add optimization summary\n            insights[\"optimization\"] = {\n                \"total_recommendations\": len(self.optimization_history),\n                \"pending_recommendations\": len(self.get_optimization_recommendations()),\n                \"last_check\": self.last_optimization_check.isoformat()\n            }\n            \n            return insights\n        \n        except Exception as e:\n            logger.error(f\"Error getting performance insights: {e}\")\n            return {\"error\": str(e)}\n    \n    @contextmanager\n    def measure_enhanced_operation(self, \n                                 operation_name: str,\n                                 expected_duration: Optional[float] = None,\n                                 memory_intensive: bool = False):\n        \"\"\"Enhanced operation measurement with optimization tracking.\"\"\"\n        start_time = time.time()\n        start_memory = psutil.Process().memory_info().rss if memory_intensive else 0\n        \n        # Use base monitor\n        with self.base_monitor.measure_operation(operation_name) as base_context:\n            try:\n                yield\n            finally:\n                # Enhanced measurement\n                duration = time.time() - start_time\n                \n                if memory_intensive:\n                    end_memory = psutil.Process().memory_info().rss\n                    memory_growth = end_memory - start_memory\n                    \n                    if memory_growth > 50 * 1024 * 1024:  # 50MB growth\n                        logger.warning(f\"High memory growth in {operation_name}: {memory_growth / 1024 / 1024:.2f}MB\")\n                \n                if expected_duration and duration > expected_duration * 1.5:\n                    logger.warning(f\"Operation {operation_name} took {duration:.2f}s (expected ~{expected_duration:.2f}s)\")\n\n# Global enhanced monitor instance\n_enhanced_monitor: Optional[EnhancedMonitor] = None\n\ndef get_enhanced_monitor() -> EnhancedMonitor:\n    \"\"\"Get global enhanced monitor instance.\"\"\"\n    global _enhanced_monitor\n    if _enhanced_monitor is None:\n        _enhanced_monitor = EnhancedMonitor()\n        _enhanced_monitor.start_enhanced_monitoring()\n    return _enhanced_monitor