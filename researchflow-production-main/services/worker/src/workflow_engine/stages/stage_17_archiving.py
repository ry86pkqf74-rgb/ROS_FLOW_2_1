"""
Stage 17: Archiving

ArchivingAgent generates preservation-ready archives that comply with
FAIR and Dublin Core, produces BagIt manifests, simulates storage
targets (S3/MinIO, Zenodo/Figshare), and plans retention/deletion.
"""

import hashlib
import json
import logging
import os
import tarfile
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

from .base_stage_agent import BaseStageAgent, BaseTool, PromptTemplate, LANGCHAIN_AVAILABLE
from ..types import StageContext, StageResult
from ..registry import register_stage

logger = logging.getLogger("workflow_engine.stages.stage_17_archivingagent")

# LangChain tool import with graceful fallback
try:
    from langchain.tools import Tool
    LC_TOOL = Tool
except Exception:  # pragma: no cover
    LC_TOOL = None

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

DUBLIN_CORE_FIELDS = [
    "title",
    "creator",
    "subject",
    "description",
    "publisher",
    "contributor",
    "date",
    "type",
    "format",
    "identifier",
    "source",
    "language",
    "relation",
    "coverage",
    "rights",
]

FAIR_CHECKLIST = [
    "findable_persistent_identifier",
    "metadata_rich",
    "accessible_open_protocol",
    "interoperable_standard_formats",
    "reusable_clear_license",
]

PREFERRED_FORMATS = ["pdf/a", "csv", "parquet", "jsonl", "tiff"]

DEFAULT_RETENTION_DAYS = 365 * 5  # 5 years

BAGIT_VERSION = "0.97"


# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------

def _safe_prior_output(context: StageContext, stage_id: int) -> Dict[str, Any]:
    result = context.previous_results.get(stage_id)
    if result and result.output:
        return result.output
    return context.prior_stage_outputs.get(stage_id, {}).get("output_data", {}) or {}


def _list_artifacts(base_path: str) -> List[str]:
    """List non-hidden files recursively under base_path."""
    artifacts: List[str] = []
    if not os.path.exists(base_path):
        return artifacts
    for root, _, files in os.walk(base_path):
        for fname in files:
            if fname.startswith("."):
                continue
            artifacts.append(os.path.join(root, fname))
    return artifacts


def _sha256_file(path: str) -> Optional[str]:
    if not os.path.exists(path):
        return None
    h = hashlib.sha256()
    try:
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                h.update(chunk)
        return h.hexdigest()
    except Exception:
        return None


def _build_dublin_core(bundle_id: str, artifacts: List[Dict[str, Any]], context: StageContext) -> Dict[str, Any]:
    now = datetime.utcnow().isoformat() + "Z"
    cfg = context.config
    manifest = {
        "dc:identifier": bundle_id,
        "dc:title": cfg.get("archive_title", f"Archive {bundle_id}"),
        "dc:creator": cfg.get("creator", "ResearchFlow System"),
        "dc:date": now,
        "dc:type": cfg.get("resource_type", "Dataset"),
        "dc:format": cfg.get("archive_format", "bagit+tar"),
        "dc:language": cfg.get("language", "en"),
        "dc:rights": cfg.get("rights", "All rights reserved"),
        "dc:description": cfg.get("description", "Research archive generated by ResearchFlow"),
        "bundle_metadata": {
            "version": "1.0",
            "generated_at": now,
            "job_id": context.job_id,
            "governance_mode": context.governance_mode,
            "artifact_count": len(artifacts),
            "artifacts": artifacts,
        },
    }
    for field in DUBLIN_CORE_FIELDS:
        key = f"dc:{field}"
        if field in cfg and key not in manifest:
            manifest[key] = cfg[field]
    return manifest


def _fair_assessment(manifest: Dict[str, Any]) -> Dict[str, Any]:
    checks = {}
    checks["findable_persistent_identifier"] = bool(manifest.get("dc:identifier"))
    checks["metadata_rich"] = len(manifest.get("bundle_metadata", {}).get("artifacts", [])) > 0
    checks["accessible_open_protocol"] = True  # assumed for simulation
    checks["interoperable_standard_formats"] = True
    checks["reusable_clear_license"] = bool(manifest.get("dc:rights"))
    score = round(sum(1 for v in checks.values() if v) / len(checks) * 10, 2)
    return {"checks": checks, "score": score}


def _bagit_paths(base_dir: str) -> Dict[str, str]:
    return {
        "bagit_txt": os.path.join(base_dir, "bagit.txt"),
        "manifest": os.path.join(base_dir, "manifest-sha256.txt"),
        "tagmanifest": os.path.join(base_dir, "tagmanifest-sha256.txt"),
        "data_dir": os.path.join(base_dir, "data"),
    }


def _create_bagit(artifacts: List[str], dest_dir: str) -> Tuple[List[str], List[str]]:
    """Create BagIt structure. Returns (errors, created_files)."""
    errors: List[str] = []
    created: List[str] = []
    paths = _bagit_paths(dest_dir)
    os.makedirs(paths["data_dir"], exist_ok=True)

    # Copy artifacts into data dir preserving relative names
    for art in artifacts:
        try:
            rel = os.path.basename(art)
            target = os.path.join(paths["data_dir"], rel)
            os.makedirs(os.path.dirname(target), exist_ok=True)
            with open(art, "rb") as src, open(target, "wb") as dst:
                dst.write(src.read())
            created.append(target)
        except Exception as e:  # pragma: no cover
            errors.append(f"copy_failed:{art}:{e}")

    # bagit.txt
    try:
        with open(paths["bagit_txt"], "w", encoding="utf-8") as f:
            f.write(f"BagIt-Version: {BAGIT_VERSION}\nTag-File-Character-Encoding: UTF-8\n")
        created.append(paths["bagit_txt"])
    except Exception as e:
        errors.append(f"bagit_txt_failed:{e}")

    # manifest-sha256
    try:
        with open(paths["manifest"], "w", encoding="utf-8") as f:
            for art in os.listdir(paths["data_dir"]):
                full = os.path.join(paths["data_dir"], art)
                checksum = _sha256_file(full)
                if not checksum:
                    errors.append(f"checksum_failed:{full}")
                    continue
                f.write(f"{checksum} data/{art}\n")
        created.append(paths["manifest"])
    except Exception as e:
        errors.append(f"manifest_failed:{e}")

    # tagmanifest-sha256 for bagit.txt and manifest
    try:
        with open(paths["tagmanifest"], "w", encoding="utf-8") as f:
            for tag_file in (paths["bagit_txt"], paths["manifest"]):
                checksum = _sha256_file(tag_file)
                if checksum:
                    rel = os.path.relpath(tag_file, dest_dir)
                    f.write(f"{checksum} {rel}\n")
                else:
                    errors.append(f"tagchecksum_failed:{tag_file}")
        created.append(paths["tagmanifest"])
    except Exception as e:
        errors.append(f"tagmanifest_failed:{e}")

    return errors, created


def _tar_bag(bag_dir: str) -> Optional[str]:
    tar_path = f"{bag_dir}.tar.gz"
    try:
        with tarfile.open(tar_path, "w:gz") as tar:
            tar.add(bag_dir, arcname=os.path.basename(bag_dir))
        return tar_path
    except Exception:  # pragma: no cover
        return None


def _simulate_s3_upload(path: str, bucket: str, prefix: str) -> Dict[str, Any]:
    key = f"{prefix}/{os.path.basename(path)}"
    uri = f"s3://{bucket}/{key}"
    return {"bucket": bucket, "key": key, "uri": uri, "signed_url": f"https://example.com/{bucket}/{key}?sig=mock"}


def _simulate_zenodo_deposition(path: str) -> Dict[str, Any]:
    deposition_id = f"zenodo-{hash(path) % 10_000}"
    return {"service": "zenodo", "deposition_id": deposition_id, "link": f"https://zenodo.org/record/{deposition_id}"}


def _simulate_figshare_deposition(path: str) -> Dict[str, Any]:
    dep_id = f"figshare-{hash(path) % 10_000}"
    return {"service": "figshare", "deposition_id": dep_id, "link": f"https://figshare.com/articles/{dep_id}"}


def _reserve_doi(bundle_id: str) -> str:
    return f"10.12345/researchflow.{bundle_id}"


def _build_prompt_template() -> PromptTemplate:
    if not LANGCHAIN_AVAILABLE:
        class _Stub:
            @classmethod
            def from_template(cls, _t):
                return type("_T", (), {"template": _t, "format": lambda s, **kw: s.template})()
        return _Stub.from_template("{input}")
    return PromptTemplate.from_template(
        "You are an archiving agent. Produce FAIR/Dublin Core metadata, BagIt manifest, "
        "retention plan, and storage recommendations from the provided context.\n"
        "{archiving_context}"
    )


def _build_archiving_tools(archiving_context: Dict[str, Any]) -> List[BaseTool]:
    if not LANGCHAIN_AVAILABLE or LC_TOOL is None:
        return []

    def format_advice(_: str) -> str:
        return json.dumps({"preferred_formats": PREFERRED_FORMATS}, indent=2)

    def doi_stub(_: str) -> str:
        return json.dumps({"reserved_doi": _reserve_doi(archiving_context.get("bundle_id", "pending"))}, indent=2)

    return [
        LC_TOOL(name="format_recommender", func=format_advice, description="Recommend preservation formats"),
        LC_TOOL(name="doi_reserver", func=doi_stub, description="Reserve a DOI (stub)"),
    ]


def _build_provenance(context: StageContext, artifacts: List[Dict[str, Any]]) -> Dict[str, Any]:
    prior_ids = sorted(context.previous_results.keys()) if context.previous_results else []
    return {
        "stage_lineage": prior_ids,
        "artifact_checksums": {a["path"]: a.get("checksum") for a in artifacts},
        "created_at": datetime.utcnow().isoformat() + "Z",
        "commit": os.getenv("GIT_COMMIT_SHA"),
    }


# ---------------------------------------------------------------------------
# Stage implementation
# ---------------------------------------------------------------------------


@register_stage
class ArchivingAgent(BaseStageAgent):
    stage_id = 17
    stage_name = "Archiving"

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._prompt_template = _build_prompt_template()

    def get_tools(self) -> List[BaseTool]:
        return _build_archiving_tools({})

    def get_prompt_template(self) -> PromptTemplate:
        return self._prompt_template

    async def execute(self, context: StageContext) -> StageResult:
        started_at = datetime.utcnow().isoformat() + "Z"
        errors: List[str] = []
        warnings: List[str] = []
        artifacts: List[str] = []

        # Gather inputs
        bundling = _safe_prior_output(context, 15)
        validation = _safe_prior_output(context, 10)
        versioning = _safe_prior_output(context, 6)

        artifact_paths = []
        if bundling.get("bundle_path"):
            artifact_paths.append(bundling["bundle_path"])
        artifact_paths += _list_artifacts(context.artifact_path)
        artifact_paths = list(sorted(set(artifact_paths)))

        artifact_entries: List[Dict[str, Any]] = []
        for path in artifact_paths:
            checksum = _sha256_file(path)
            artifact_entries.append({
                "path": path,
                "checksum": checksum,
                "format": os.path.splitext(path)[1].lstrip(".").lower(),
                "size_bytes": os.path.getsize(path) if os.path.exists(path) else None,
            })
            if checksum is None:
                warnings.append(f"Checksum missing for {path}")

        bundle_id = context.config.get("bundle_id", f"archive-{context.job_id}")
        dublin_core_manifest = _build_dublin_core(bundle_id, artifact_entries, context)
        fair = _fair_assessment(dublin_core_manifest)

        # BagIt creation
        bag_dir = os.path.join(context.artifact_path, f"{bundle_id}_bag")
        bag_errors, bag_created = _create_bagit(artifact_paths, bag_dir)
        artifacts.extend(bag_created)
        if bag_errors:
            warnings.extend(bag_errors)

        bag_tar = _tar_bag(bag_dir)
        if bag_tar:
            artifacts.append(bag_tar)
        else:
            warnings.append("Failed to create BagIt tarball.")

        # Storage simulation
        storage_cfg = context.config.get("storage", {})
        bucket = storage_cfg.get("bucket", "researchflow-archive")
        prefix = storage_cfg.get("prefix", f"archives/{bundle_id}")
        storage_record = _simulate_s3_upload(bag_tar or bag_dir, bucket, prefix)

        deposition = None
        dep_target = storage_cfg.get("deposition", "zenodo")
        if dep_target == "zenodo":
            deposition = _simulate_zenodo_deposition(bag_tar or bag_dir)
        elif dep_target == "figshare":
            deposition = _simulate_figshare_deposition(bag_tar or bag_dir)

        doi = _reserve_doi(bundle_id) if context.config.get("reserve_doi", True) else None

        # Retention and legal hold
        retention_days = context.config.get("retention_days", DEFAULT_RETENTION_DAYS)
        legal_hold = context.config.get("legal_hold", False)
        deletion_at = None if legal_hold else (datetime.utcnow() + timedelta(days=retention_days)).isoformat() + "Z"

        provenance = _build_provenance(context, artifact_entries)

        archive_manifest = {
            "bundle_id": bundle_id,
            "dublin_core": dublin_core_manifest,
            "fair": fair,
            "artifacts": artifact_entries,
            "bagit": {
                "path": bag_dir,
                "tar": bag_tar,
                "status": "ok" if not bag_errors else "warning",
                "errors": bag_errors,
            },
            "storage": storage_record,
            "deposition": deposition,
            "doi": doi,
            "retention": {
                "days": retention_days,
                "legal_hold": legal_hold,
                "deletion_scheduled_for": deletion_at,
            },
            "provenance": provenance,
            "validation": validation,
            "versioning": versioning,
        }

        # Persist artifacts
        manifest_path = os.path.join(context.artifact_path, f"archive_manifest_{context.job_id}.json")
        fair_path = os.path.join(context.artifact_path, f"fair_checklist_{context.job_id}.json")
        os.makedirs(context.artifact_path, exist_ok=True)
        try:
            with open(manifest_path, "w", encoding="utf-8") as f:
                json.dump(archive_manifest, f, indent=2, ensure_ascii=False)
            artifacts.append(manifest_path)
            with open(fair_path, "w", encoding="utf-8") as f:
                json.dump(fair, f, indent=2, ensure_ascii=False)
            artifacts.append(fair_path)
        except Exception as e:  # pragma: no cover
            warnings.append(f"Failed to persist manifests: {e}")

        # Governance mode note
        if context.governance_mode == "DEMO":
            warnings.append("Running in DEMO mode; storage and DOI are simulated.")

        status = "failed" if errors else "completed"

        return self.create_stage_result(
            context=context,
            status=status,
            started_at=started_at,
            output=archive_manifest,
            artifacts=artifacts,
            errors=errors,
            warnings=warnings,
            metadata={
                "bagit_status": "ok" if not bag_errors else "warning",
                "checksum_status": "ok" if not warnings else "check_warnings",
                "retention_effective_until": deletion_at,
                "doi": doi,
            },
        )
