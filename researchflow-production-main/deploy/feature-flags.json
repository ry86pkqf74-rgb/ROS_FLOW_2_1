{
  "version": "1.0.0",
  "lastUpdated": "2026-01-30T00:00:00Z",
  "flags": [
    {
      "name": "ai_inference",
      "description": "Enable AI inference through Ollama and Triton",
      "rollout": 0,
      "enabled": true,
      "rolloutStrategy": "canary",
      "rolloutPercentages": [10, 25, 50, 75, 100],
      "targetPercentage": 100,
      "dependencies": [],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["ai", "core", "phase-9"]
    },
    {
      "name": "vector_search",
      "description": "Enable semantic vector search through FAISS",
      "rollout": 0,
      "enabled": true,
      "rolloutStrategy": "canary",
      "rolloutPercentages": [10, 25, 50, 75, 100],
      "targetPercentage": 100,
      "dependencies": ["ai_inference"],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["ai", "search", "phase-9"]
    },
    {
      "name": "semantic_cache",
      "description": "Enable semantic caching for inference results",
      "rollout": 0,
      "enabled": true,
      "rolloutStrategy": "canary",
      "rolloutPercentages": [10, 25, 50, 75, 100],
      "targetPercentage": 100,
      "dependencies": ["ai_inference"],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["ai", "cache", "performance", "phase-9"]
    },
    {
      "name": "triton_gpu_acceleration",
      "description": "Enable GPU acceleration in Triton Inference Server",
      "rollout": 0,
      "enabled": true,
      "rolloutStrategy": "canary",
      "rolloutPercentages": [25, 50, 100],
      "targetPercentage": 100,
      "dependencies": ["ai_inference"],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["ai", "gpu", "performance", "phase-9"]
    },
    {
      "name": "batch_inference",
      "description": "Enable batch inference processing",
      "rollout": 0,
      "enabled": true,
      "rolloutStrategy": "canary",
      "rolloutPercentages": [25, 50, 100],
      "targetPercentage": 100,
      "dependencies": ["ai_inference"],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["ai", "batch", "performance", "phase-9"]
    },
    {
      "name": "advanced_monitoring",
      "description": "Enable advanced AI metrics and monitoring",
      "rollout": 100,
      "enabled": true,
      "rolloutStrategy": "immediate",
      "rolloutPercentages": [100],
      "targetPercentage": 100,
      "dependencies": [],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["monitoring", "observability", "phase-9"]
    },
    {
      "name": "cost_optimization",
      "description": "Enable cost optimization features",
      "rollout": 50,
      "enabled": true,
      "rolloutStrategy": "canary",
      "rolloutPercentages": [25, 50, 75, 100],
      "targetPercentage": 100,
      "dependencies": ["ai_inference"],
      "createdAt": "2026-01-30T00:00:00Z",
      "updatedAt": "2026-01-30T00:00:00Z",
      "tags": ["cost", "optimization", "phase-9"]
    },
    {
      "name": "mercury_coder",
      "description": "Enable Mercury Coder (Inception Labs) ultra-fast diffusion LLM",
      "rollout": 100,
      "enabled": true,
      "rolloutStrategy": "immediate",
      "rolloutPercentages": [100],
      "targetPercentage": 100,
      "dependencies": ["ai_inference"],
      "createdAt": "2026-02-03T00:00:00Z",
      "updatedAt": "2026-02-03T00:00:00Z",
      "tags": ["ai", "mercury", "code-completion", "realtime", "phase-10"]
    },
    {
      "name": "mercury_realtime",
      "description": "Enable Mercury realtime mode for near-instant responses",
      "rollout": 100,
      "enabled": true,
      "rolloutStrategy": "immediate",
      "rolloutPercentages": [100],
      "targetPercentage": 100,
      "dependencies": ["mercury_coder"],
      "createdAt": "2026-02-03T00:00:00Z",
      "updatedAt": "2026-02-03T00:00:00Z",
      "tags": ["ai", "mercury", "realtime", "low-latency", "phase-10"]
    },
    {
      "name": "mercury_structured_outputs",
      "description": "Enable Mercury structured outputs with JSON schema enforcement",
      "rollout": 100,
      "enabled": true,
      "rolloutStrategy": "immediate",
      "rolloutPercentages": [100],
      "targetPercentage": 100,
      "dependencies": ["mercury_coder"],
      "createdAt": "2026-02-03T00:00:00Z",
      "updatedAt": "2026-02-03T00:00:00Z",
      "tags": ["ai", "mercury", "structured-outputs", "json-schema", "phase-10"]
    },
    {
      "name": "mercury_auto_route",
      "description": "Enable automatic routing to Mercury for low-latency tasks",
      "rollout": 100,
      "enabled": true,
      "rolloutStrategy": "immediate",
      "rolloutPercentages": [100],
      "targetPercentage": 100,
      "dependencies": ["mercury_coder", "mercury_realtime"],
      "createdAt": "2026-02-03T00:00:00Z",
      "updatedAt": "2026-02-03T00:00:00Z",
      "tags": ["ai", "mercury", "auto-routing", "orchestration", "phase-10"]
    },
    {
      "name": "mercury_notifications",
      "description": "Enable Mercury usage notifications to Slack/Notion/Linear",
      "rollout": 100,
      "enabled": true,
      "rolloutStrategy": "immediate",
      "rolloutPercentages": [100],
      "targetPercentage": 100,
      "dependencies": ["mercury_coder"],
      "createdAt": "2026-02-03T00:00:00Z",
      "updatedAt": "2026-02-03T00:00:00Z",
      "tags": ["ai", "mercury", "notifications", "integrations", "phase-10"]
    }
  ]
}
